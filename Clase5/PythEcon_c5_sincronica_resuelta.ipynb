{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curso de Python para Economistas\n",
    "## Bienvenidos a la clase 5\n",
    "\n",
    "### Anuncios y temario para hoy\n",
    "\n",
    "- Pull de GitHub\n",
    "- Cuestionario dificultad tarea\n",
    "- Respuestas del cuestionario de los videos\n",
    "- Dudas de la teoría\n",
    "- Ejercicio práctico: Analisis de sentimiento de tweets de Joe Biden pre y post elecciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuestionario dificultad Tarea\n",
    "https://forms.gle/yCLHV93TFNLd9Ug86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respuestas a las preguntas de los videos:\n",
    "https://docs.google.com/forms/d/e/1FAIpQLSfb7zlEHh3gllD3iUUWwPCMwyQPd0Z04F3XpArSKQzPkYvGHA/viewform?usp=pp_url&entry.1318485631=Respuesta+Satisfactoria+(Successful+responses)&entry.330305262=Una+celda+de+una+tabla&entry.871102831=Tabula&entry.871102831=PyPDF2&entry.871102831=Minecart&entry.2076477619=La+puedo+trabajar+desde+python+con+el+paquete+psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de sentimiento de tweets de Joe Biden pre y post elecciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.2.2-cp39-cp39-macosx_10_9_x86_64.whl (160 kB)\n",
      "\u001b[K     |████████████████████████████████| 160 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /Users/belenmichel/opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/belenmichel/opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: pillow in /Users/belenmichel/opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/belenmichel/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/belenmichel/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/belenmichel/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/belenmichel/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/belenmichel/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/belenmichel/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/belenmichel/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.8.2.2\n",
      "Requirement already satisfied: Pillow in /Users/belenmichel/opt/anaconda3/lib/python3.9/site-packages (9.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/belenmichel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importamos los paquetes a utilizar\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "!{sys.executable} -m pip install Pillow\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "#Si alguien quiere trabajar con alguna cuenta en español podrían instalar \n",
    "#sentiment_analysis_spanish y utilizarlo en reemplado de TextBlob:\n",
    "#!{sys.executable} -m pip install sentiment_analysis_spanish\n",
    "#from sentiment_analysis_spanish import sentiment_analysis\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# Si trabajan en español pueden crearse una lista de stopwords con las de este link:\n",
    "# https://github.com/xiamx/node-nltk-stopwords/blob/master/data/stopwords/spanish\n",
    "# También usaremos string.punctuation. Si trabajan en español podrian agregarle  \n",
    "# ¿ y ¡ . En ingles incluye lo siguiente: !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear con algun editor de texto (ej. Sublime) un archivo llamado `twitter_keys.txt` dentro de la carpeta `clase5` y guardar las 4 claves, una por línea, en el siguiente orden:\n",
    "- API key\n",
    "- API key secret\n",
    "- Access token\n",
    "- Access token secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos variables que contienen nuestas claves de autenticación con la API\n",
    "with open(\"twitter_keys.txt\") as tw_k: \n",
    "    consumer_key = tw_k.readline().strip()\n",
    "    consumer_secret = tw_k.readline().strip()\n",
    "    access_key = tw_k.readline().strip()\n",
    "    access_secret = tw_k.readline().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Le pasamos nuestras credenciales de twitter a tweepy\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este link pueden explorar detalles del metodo user_timeline: https://docs.tweepy.org/en/stable/api.html?highlight=user_timeline#tweepy.API.user_timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tweets(screen_name, start_date):\n",
    "    '''\n",
    "    Esta funcion recibe el nombre de la persona de quien queremos extraer los \n",
    "    tweets y devuelve una lista con todos los tweets y sus datos\n",
    "    Input: \n",
    "      screen_name (str): el nombre de la persona en twitter\n",
    "    Output:\n",
    "      all_tweets (lista): lista con todos los tweets extraidos\n",
    "    '''\n",
    "    # Solicitamos los 200 tweets mas recientes (200 es el maximo permitido en count)\n",
    "    new_tweets = api.user_timeline(screen_name=screen_name, \n",
    "                                   tweet_mode=\"extended\", count=200)\n",
    "    # Creo una lista para almacenar TODOS los tweets y agrego los recién extraidos\n",
    "    all_tweets = []\n",
    "    all_tweets.extend(new_tweets)\n",
    "    # guardo el id del ultimo tweet extraído \n",
    "    oldest = all_tweets[-1].id \n",
    "    \n",
    "    # Extraigo tweets de a 200 hasta que no haya más\n",
    "    while len(new_tweets) > 0 and all_tweets[-1].created_at > start_date:\n",
    "        # Solicito 200 tweets mas y los agrego a la lista de 'all_tweets'\n",
    "        new_tweets = api.user_timeline(screen_name=screen_name, count=200,\n",
    "                                       tweet_mode=\"extended\", max_id=oldest-1)\n",
    "        all_tweets.extend(new_tweets)\n",
    "        # Actualizo el id del ultimo tweet extraído\n",
    "        oldest = all_tweets[-1].id \n",
    "        print(\"Hasta ahora se han extraido %s tweets\" % len(all_tweets))\n",
    "\n",
    "    return all_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Unauthorized",
     "evalue": "401 Unauthorized\n32 - Could not authenticate you.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnauthorized\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extraemos los tweets desde unos días antes de las elecciones del \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 3 de Noviembre de 2020\u001b[39;00m\n\u001b[1;32m      3\u001b[0m date_before_elections \u001b[38;5;241m=\u001b[39m datetime(\u001b[38;5;241m2020\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m all_tweets_biden \u001b[38;5;241m=\u001b[39m \u001b[43mget_all_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJoeBiden\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_before_elections\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mget_all_tweets\u001b[0;34m(screen_name, start_date)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mEsta funcion recibe el nombre de la persona de quien queremos extraer los \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mtweets y devuelve una lista con todos los tweets y sus datos\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m  all_tweets (lista): lista con todos los tweets extraidos\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Solicitamos los 200 tweets mas recientes (200 es el maximo permitido en count)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m new_tweets \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_timeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscreen_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscreen_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mtweet_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextended\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Creo una lista para almacenar TODOS los tweets y agrego los recién extraidos\u001b[39;00m\n\u001b[1;32m     14\u001b[0m all_tweets \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tweepy/api.py:33\u001b[0m, in \u001b[0;36mpagination.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tweepy/api.py:46\u001b[0m, in \u001b[0;36mpayload.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayload_list\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m payload_list\n\u001b[1;32m     45\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayload_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m payload_type\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tweepy/api.py:571\u001b[0m, in \u001b[0;36mAPI.user_timeline\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;129m@pagination\u001b[39m(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;129m@payload\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21muser_timeline\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;124;03m\"\"\"user_timeline(*, user_id, screen_name, since_id, count, max_id, \\\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;124;03m                     trim_user, exclude_replies, include_rts)\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m    https://developer.twitter.com/en/docs/twitter-api/v1/tweets/timelines/api-reference/get-statuses-user_timeline\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstatuses/user_timeline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscreen_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msince_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrim_user\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexclude_replies\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minclude_rts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tweepy/api.py:257\u001b[0m, in \u001b[0;36mAPI.request\u001b[0;34m(self, method, endpoint, endpoint_parameters, params, headers, json_payload, parser, payload_list, payload_type, post_data, files, require_auth, return_cursors, upload_api, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequest(resp)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[0;32m--> 257\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Unauthorized(resp)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Forbidden(resp)\n",
      "\u001b[0;31mUnauthorized\u001b[0m: 401 Unauthorized\n32 - Could not authenticate you."
     ]
    }
   ],
   "source": [
    "# Extraemos los tweets desde unos días antes de las elecciones del \n",
    "# 3 de Noviembre de 2020\n",
    "date_before_elections = datetime(2020, 10, 16, 0, 0, 0)\n",
    "all_tweets_biden = get_all_tweets(\"JoeBiden\", date_before_elections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veamos la lista con los primeros 5 objetos de tweepy\n",
    "all_tweets_biden[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos un solo tweet \n",
    "all_tweets_biden[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos el texto de un solo tweet \n",
    "all_tweets_biden[8].full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 1: \n",
    "En la siguiente función construyan un loop que extraiga id_str, created_at, full_text, retweeted,  favorite_count, in_reply_to_screen_name de cada tweet y lo guarde en una lista. Esa lista se debe agregar al final de la lista all_tweets_selection para luego construir un df con la lista de listas (las listas internas serán las filas del df). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweets_text(all_tweets, csv_file=None):\n",
    "    '''\n",
    "    Esta función guarda los tweets en un data frame y si se especifica un \n",
    "    archivo csv tambien se guardaran ahí \n",
    "    Input:\n",
    "        all_tweets (lista): lista con tweets y sus datos\n",
    "        csv_file ('str'): nombre del archivo csv\n",
    "    Output:\n",
    "        df_all_tweets (df): tweets ordenados en una tabla con datos seleccinados\n",
    "    '''\n",
    "    all_tweets_selection = []\n",
    "    \n",
    "    # Construyan un loop que extraiga id_str, created_at, full_text, retweeted, \n",
    "    # favorite_count, in_reply_to_screen_name de cada tweet y lo guarde en una\n",
    "    # lista. Esa lista se debe agregar al final de la lista all_tweets_selection\n",
    "    # para luego construir un df con la lista de listas (las listas internas \n",
    "    # serán las filas del df). \n",
    "\n",
    "    for tweet in all_tweets:\n",
    "        one_tweet = [tweet.id_str, tweet.created_at, tweet.full_text, \n",
    "                     tweet.retweeted, tweet.favorite_count, \n",
    "                     tweet.in_reply_to_screen_name]\n",
    "        all_tweets_selection.append(one_tweet)\n",
    "    \n",
    "    # Hasta aqui su código    \n",
    "    df_all_tweets = pd.DataFrame(all_tweets_selection)\n",
    "    df_all_tweets.columns = ['id_str', 'created_at', 'text', 'retweeted',\n",
    "                            'favorite_count', 'in_reply_to_screen_name']\n",
    "    if csv_file:\n",
    "        df_all_tweets.to_csv(csv_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    return df_all_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 1b: \n",
    "Utilicen la función save_tweets_text para construir una tabla con todos los tweets y guardenlo en una archivo llamado \"tweets.csv\". Por ultimo visualicen algunas filas de la tabla. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_tweets = save_tweets_text(all_tweets_biden, \"tweets.csv\")\n",
    "df_all_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Ejercicio 1b: \n",
    " Hagan print del texto de algunos tweets y luego piensen:\n",
    " \n",
    " 1) Les parece que podríamos hacer un análisis de sentimiento de estos tweets como están?\n",
    " \n",
    " 2) Porque?\n",
    " \n",
    " 3) Que errores creen que podriamos tener al trabajar con el texto tal como lo descargamos de twitter?\n",
    " \n",
    " 4) Que transformaciones le harían al texto antes de meterlo en el análisis (mencionen al menos 5 cosas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoticones contentos\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Emoticones Tristes\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "# Combinamos emoticones contentos y tristes\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # simbolos & pictogramas\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transporte & simbolos mapas\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # banderas (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\\U0001F600\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Esta función limpia el texto de un tweet. Elimina caracteres especificos que\n",
    "    se utilizan en twitter como los de re-tweets, los links y otros Non-ASCII.\n",
    "    Devuelve el texto \"limpio\".\n",
    "    Input:\n",
    "        tweet (str): Texto del tweet original\n",
    "    Output:\n",
    "        tweet (str): Texto del tweet limpiado\n",
    "    '''   \n",
    "    #Saco los links\n",
    "    tweet = re.sub(r'https.*', '', tweet)\n",
    "    #Elimino caracteres de re-tweets   \n",
    "    tweet = re.sub(r'^RT .*:', '', tweet)\n",
    "    tweet = re.sub(r'@\\S+', '', tweet)\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "    #Reemplazo caracteres non-ASCII con espacio\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    \n",
    "    return tweet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Este es un tweet sucio:\n",
    "tweet = df_all_tweets['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Este es un tweet limpio:\n",
    "tweet_cleaned = clean_tweet(df_all_tweets.iloc[1]['text'])\n",
    "tweet_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de Sentimiento\n",
    "https://textblob.readthedocs.io/en/dev/api_reference.html#module-textblob.en.sentiments\n",
    "- Polarity: Negative (-1.0) vs. Positive (1.0)\n",
    "- Subjectivity: Objective (0.0) vs. Subjective (1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el sentimiento con el metodo TextBlob\n",
    "blob = TextBlob(tweet_cleaned)\n",
    "Sentiment = blob.sentiment\n",
    "polarity = Sentiment.polarity\n",
    "subjectivity = Sentiment.subjectivity  \n",
    "print(\"La polaridad de este tweet es :\", polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 2:\n",
    "Construyamos un loop que nos permita limpiar todos los strings y concatenarlos. Al tenerlos todos concatenados será más facil utilizarlos en un grafico, como por ejemplo una nube de pablabras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_cleaned = ''\n",
    "all_tweets = ''\n",
    "for index, row in df_all_tweets.iterrows():\n",
    "    all_tweets += row.text\n",
    "    clean_text = clean_tweet(row.text)\n",
    "    all_tweets_cleaned += clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos una imagen de nube de palabras \n",
    "# wordcloud = WordCloud().generate(all_tweets_cleaned)\n",
    "\n",
    "wordcloud = WordCloud().generate(all_tweets) #prueben incluir: max_font_size=40\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(background_color = 'white', stopwords = stopwords.words('english')).generate(all_tweets_cleaned)\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Image.open('upvote.png') as im:\n",
    "    im.rotate(45).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import image to np.array\n",
    "mask = np.array(Image.open('upvote.png'))\n",
    "\n",
    "# Generate wordcloud\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, background_color='white', \n",
    "                      collocations=False, stopwords = stopwords.words('english'), \n",
    "                      mask=mask).generate(all_tweets_cleaned)\n",
    "\n",
    "plt.figure(figsize=(40, 30))\n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: La imagen del dedo la tomé como idea de este blog https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 3:\n",
    "Creen una función que limpie texto en general. Elimina emoticones, emojis, palabras vacías (también llamadas stop words en la libreria de nltk), links, puntuaciones, indicaciones de retweets, etc. Esta función dejara el texto limpio y solo con las palabras que pueden agregar mayor valor para el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, is_tweet=False):\n",
    "    '''\n",
    "    Esta función limpia el texto del tweet. Elimina emoticones, emojis, palabras\n",
    "    vacías (también llamadas stop words en la libreria de nltk), links, puntuaciones\n",
    "    indicaciones de retweets, etc. Para dejar en el texto solo las palabras con \n",
    "    mayor contenido.\n",
    "    Input:\n",
    "        text (str): Texto original\n",
    "        is_tweet (bool): si el texto es un tweet este parametro debe setearse a \n",
    "                         true para que también se limpie el texto con los \n",
    "                         caracteres más especificos de twitter (usa clean_tweet) \n",
    "    Output:\n",
    "        text (str): Texto limpiado\n",
    "    '''\n",
    "    #Si es un tweet elimino caracteres especificos que de twitter (retweets, links)\n",
    "    if is_tweet:\n",
    "        text = clean_tweet(text)\n",
    "    \n",
    "    #Saco los emoji\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    #Separo los tweets in tokens\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    #Obtengo set de palabras vacias para luego eliminarlas\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #loop por las condiciones\n",
    "    filtered_tweet = []\n",
    "    for w in word_tokens:\n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in emoticons and \\\n",
    "           w not in string.punctuation:\n",
    "            filtered_tweet.append(w)\n",
    "\n",
    "    \n",
    "    return (' '.join(filtered_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_tweets['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(df_all_tweets['text'][1], is_tweet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
